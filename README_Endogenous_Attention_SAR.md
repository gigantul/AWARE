# Endogenous Attention-Weighted SAR: A Clean and Efficient Framework for Semantic-Aware Uncertainty in LLMs

## ğŸ“Œ Summary
This repository re-implements and extends the core methodology of "Shifting Attention to Relevance (SAR)" by Duan et al., offering a:

- âœ… **Modular, memory-efficient SAR pipeline**
- âœ… **Endogenous attention-weighted token importance** (instead of SBERT)
- âœ… **Native support for multiple uncertainty measures** (SAR, PE, SE, LN-PE, etc.)
- âœ… Clean integration with HuggingFace and minimal external dependencies

> This version serves as a reproducible and extensible base for research in uncertainty estimation, semantic relevance, and token-level confidence in LLM generations.

---

## ğŸ”¬ Key Contributions
- **Main Pipeline** (`main_pipeline.py`): Unified, batch-based processing for any dataset (SciQ, TriviaQA, CoQA)
- **Plug-in Uncertainty Modules**: Supports SAR, token-SAR, sentence-SAR, PE, SE, semantic entropy
- **Replaces SBERT with Endogenous Attention Weights**: Token importance derived from model internals (attention heads), increasing interpretability and efficiency
- **Reproducible Figures & Tables**: Scripts replicate Duan et al.'s Table 1â€“4, Figure 5â€“6

---

## ğŸ“ Structure

```bash
â”œâ”€â”€ main_pipeline.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ sciq_loader.py
â”‚   â”œâ”€â”€ coqa_loader.py
â”‚   â””â”€â”€ triviaqa_loader.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ generator.py
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ likelihoods.py
â”‚   â”œâ”€â”€ uncertainty.py
â”‚   â”œâ”€â”€ similarity.py
â”‚   â””â”€â”€ correctness.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_sar_pipeline.sh
â””â”€â”€ results/  # output CSVs + pickles
```

---

## ğŸš€ How to Run

```bash
bash scripts/run_sar_pipeline.sh
```
This will:
- Run SAR-style generation and analysis for `sciq` and `triviaqa`
- Compute uncertainty metrics
- Generate correctness labels

Output files (CSV + pickle) are saved under `results/` and `output_dir/`.

---

## ğŸ“Š Replicated Tables and Figures

| Table/Figure | Covered? | Location |
|--------------|----------|----------|
| Table 1 (SciQ Accuracy)   | âœ… | `uncertainty.py` output
| Table 2 (TriviaQA)        | âœ… | `uncertainty.py`
| Table 3/4 (AUC/Correlation) | âœ… | `uncertainty.py`
| Figure 5 (Distribution)   | âš ï¸ (script optional) | TBD
| Figure 6 (Token entropy/importance) | âš ï¸ (visual script WIP) | TBD

---

## ğŸ‘¥ Authorship & Contributions

**Original Code & Pipeline:** [Your Name Here]  
**Paper Reference:** Duan et al., "Shifting Attention to Relevance: A Framework for Evaluating Semantic Uncertainty in LLMs" (2023)

If this code is extended in future work (e.g., additional datasets, interpretability tools, or visualizations), **new contributors may be added to later arXiv versions or journal submissions**.

---

## ğŸ“ arXiv Posting Plan
- Post v1 as sole author
- Invite collaborators to contribute to v2 (e.g., new metrics, datasets, visualization)
- Update authorship with consent upon arXiv resubmission

---

## ğŸ“® Contact
Interested in collaborating or reusing this? Feel free to reach out.

- Email: your.email@example.com
- GitHub: [your-handle]

---

## ğŸ“š Citation
BibTeX coming after arXiv upload. Stay tuned.
