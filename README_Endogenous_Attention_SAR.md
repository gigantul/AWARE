# AWARE: Attention-Weighted and Reduced Entropy
### A Clean and Efficient Framework for Semantic-Aware Uncertainty in LLMs

## ğŸ“Œ Summary
This repository re-implements and extends the core methodology of *"Shifting Attention to Relevance (SAR)"* by Duan et al., introducing:

- âœ… **A modular, memory-efficient SAR-based pipeline**
- âœ… **Endogenous attention-weighted token importance** (no external SBERT)
- âœ… **Dimensionally-collapsed logit entropy** for semantically-refined uncertainty
- âœ… **Support for multiple uncertainty metrics** (SAR, PE, SE, LN-PE, and more)
- âœ… **Minimal dependencies with HuggingFace-native integration**

> This implementation serves as a reproducible and extensible foundation for research on uncertainty estimation, semantic relevance, and token-level confidence in LLMs â€” now refined into **AWARE**, a novel measure based on Attention-Weighted and Reduced Entropy.

---

## ğŸ”¬ Key Contributions
- **Main Pipeline** (`main_pipeline.py`): Unified, batch-based processing for datasets like SciQ, TriviaQA, and CoQA
- **Plug-in Uncertainty Modules**: Including SAR, token-SAR, sentence-SAR, PE, SE, semantic entropy, and more
- **Endogenous Token Importance via Attention Weights**: Replacing SBERT with internal model attention for interpretable and efficient weighting
- **Collapsed Logit Space for Semantic Refinement**: Reducing output vocabulary to task-relevant dimensions for more accurate entropy calculation
- **Reproducible Experiments**: Scripts replicate Duan et al.â€™s Table 1â€“4 and Figure 5â€“6

---

## ğŸ“ Repository Structure

```bash
â”œâ”€â”€ main_pipeline.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ sciq_loader.py
â”‚   â”œâ”€â”€ coqa_loader.py
â”‚   â””â”€â”€ triviaqa_loader.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ generator.py
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ likelihoods.py
â”‚   â”œâ”€â”€ uncertainty.py
â”‚   â”œâ”€â”€ similarity.py
â”‚   â””â”€â”€ correctness.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_all_uncertainty.sh
â””â”€â”€ results/  # output CSVs + pickles
```

---

## ğŸš€ Getting Started

To reproduce results:

```bash
bash scripts/run_sar_pipeline.sh
```

This will:
- Run generation + uncertainty estimation on SciQ and TriviaQA
- Compute uncertainty metrics across methods
- Generate correctness labels

Outputs will be saved to `results/` and `output_dir/`.

---

## ğŸ“Š Replicated Experiments

| Table/Figure | Status | Location |
|--------------|--------|----------|
| Table 1: SciQ Accuracy            | âœ… | `uncertainty.py`
| Table 2: TriviaQA Accuracy       | âœ… | `uncertainty.py`
| Table 3/4: AUC / Correlation     | âœ… | `uncertainty.py`
| Figure 5: Uncertainty Distributions | âš ï¸ (optional script) | TBD
| Figure 6: Token Entropy & Importance | âš ï¸ (WIP visualization) | TBD

---

## ğŸ‘¥ Authorship & Contributions

**Core Pipeline & Code:** Kwanhee Lee

**Original Methodology Reference:**  
Duan et al., *"Shifting Attention to Relevance: A Framework for Evaluating Semantic Uncertainty in LLMs"* (2023)

> Future contributions (e.g., new datasets, entropy variants, visualization tools) are welcome and may be reflected in updated arXiv or journal versions.

---

## ğŸ“ arXiv Posting Plan

- v1: Solo author submission with AWARE methodology + SAR replication
- v2+: Open to collaborators (metrics, interpretability, applications)
- Authorship will reflect meaningful contributions with prior consent

---

## ğŸ“® Contact

Interested in collaborating, reusing, or building on this work?

- Email: gigantul@korea.ac.kr
- GitHub: https://github.com/gigantul/endogenous_attention_sar

---

## ğŸ“š Citation

BibTeX and full citation will be included after the first arXiv upload. Stay tuned!
